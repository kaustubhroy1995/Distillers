[TEACHER_CONFIG]
num_hidden_layers = 12
hidden_size = 768
num_attention_heads = 6
hidden_act = gelu
max_position_embeddings = 512
model_type = bert
pad_token_id = 0
position_embedding_type = absolute
type_vocab_size = 2
use_cache = true
vocab_size = 30522
intermediate_size = 3072
initializer_range = 0.02